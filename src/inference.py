"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è submission_result.csv.

–ü–æ—Å—Ç–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥ –¥–µ–ª–∞–µ—Ç:
- –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—é subtoken ‚Üí word-level —Å–ø–∞–Ω–æ–≤;
- —É–¥–∞–ª–µ–Ω–∏–µ "—Å—É—â–Ω–æ—Å—Ç–µ–π" –Ω–∞ –ø—Ä–æ–±–µ–ª–∞—Ö/–ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏;
- —Å–ª–∏—è–Ω–∏–µ –≤–ª–æ–∂–µ–Ω–Ω—ã—Ö/–ø–µ—Ä–µ—Å–µ–∫–∞—é—â–∏—Ö—Å—è —Å–ø–∞–Ω–æ–≤;
- –∫–æ—Ä—Ä–µ–∫—Ç–Ω—É—é —Ä–∞—Å—Å—Ç–∞–Ω–æ–≤–∫—É BIO –Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å–ª–æ–≤ –æ–¥–Ω–æ–≥–æ —Ç–∏–ø–∞.
"""

import os
import re
import torch
import pandas as pd
from data_processing import preprocess_query, tokenizer
from model import NERModel, id2label

# –ü—É—Ç–∏
DATA_DIR = "data"
INPUT_SUBMISSION = os.path.join(DATA_DIR, "submission.csv")          # –∏—Å—Ö–æ–¥–Ω—ã–π —Ñ–∞–π–ª (—Å—Ç–æ–ª–±–µ—Ü sample)
OUTPUT_SUBMISSION = os.path.join(DATA_DIR, "submission_result.csv")  # —Ä–µ–∑—É–ª—å—Ç–∞—Ç (search_query + –Ω–æ–≤–∞—è annotation)
CKPT_PATH = os.path.join("model_checkpoint", "ner_model.pth")

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# =====================
# üîπ –†–µ–≥—É–ª—è—Ä–∫–∏ –∏ —Ö–µ–ª–ø–µ—Ä—ã
# =====================
WORD_RE = re.compile(r"[0-9A-Za-z–ê-–Ø–∞-—è–Å—ë]+")   # —Å–ª–æ–≤–æ = –±—É–∫–≤—ã/—Ü–∏—Ñ—Ä—ã (RU/EN)
NON_WORD_RE = re.compile(r"^\W+$", re.UNICODE)  # —Ç–æ–ª—å–∫–æ –ø—Ä–æ–±–µ–ª—ã/–ø—É–Ω–∫—Ç—É–∞—Ü–∏—è


def split_entity_into_bio_spans(text: str, start: int, end: int, ent_type: str):
    """
    –†–∞–∑–±–∏–≤–∞–µ—Ç (start,end) –Ω–∞ word-level —Å–ø–∞–Ω—ã –ø–æ WORD_RE.
    –ü–µ—Ä–≤—ã–π ‚Üí B-—Ç–∏–ø, –æ—Å—Ç–∞–ª—å–Ω—ã–µ ‚Üí I-—Ç–∏–ø.
    –ï—Å–ª–∏ –≤–Ω—É—Ç—Ä–∏ –Ω–µ—Ç —Å–ª–æ–≤, –≤–µ—Ä–Ω—ë—Ç [].
    """
    chunk = text[start:end]
    word_spans = [(m.start() + start, m.end() + start) for m in WORD_RE.finditer(chunk)]
    if not word_spans:
        return []
    spans = []
    for i, (s, e) in enumerate(word_spans):
        tag = "B" if i == 0 else "I"
        spans.append((s, e, f"{tag}-{ent_type}"))
    return spans


def merge_overlapping_spans(spans):
    """
    –°–ª–∏–≤–∞–µ—Ç –ø–µ—Ä–µ—Å–µ–∫–∞—é—â–∏–µ—Å—è –∏–ª–∏ –≤–ª–æ–∂–µ–Ω–Ω—ã–µ —Å–ø–∞–Ω—ã –æ–¥–Ω–æ–≥–æ —Ç–∏–ø–∞.
    –ù–∞–ø—Ä–∏–º–µ—Ä: [(0,1,'B-TYPE'), (0,11,'I-TYPE')] ‚Üí [(0,11,'B-TYPE')]
    """
    if not spans:
        return []

    spans = sorted(spans, key=lambda x: (x[0], x[1]))
    merged = []
    for s, e, tag in spans:
        ent_type = tag.split("-", 1)[-1]
        if not merged:
            merged.append([s, e, ent_type])
            continue

        last_s, last_e, last_type = merged[-1]
        if ent_type == last_type and s >= last_s and e <= last_e:
            continue  # —Ç–µ–∫—É—â–∏–π –≤–Ω—É—Ç—Ä–∏ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ
        if ent_type == last_type and s <= last_e:
            merged[-1][1] = max(last_e, e)  # —Ä–∞—Å—à–∏—Ä—è–µ–º
        else:
            merged.append([s, e, ent_type])

    return [(s, e, f"B-{t}") for s, e, t in merged]


def postprocess_to_word_level_bio(text: str, raw_entities):
    """
    –ü—Ä–µ–≤—Ä–∞—â–∞–µ—Ç subtoken-level —Å—É—â–Ω–æ—Å—Ç–∏ –≤ word-level BIO —Å —á–∏—Å—Ç–∫–æ–π.
    """
    # 1) –†–∞–∑–±–∏–≤–∞–µ–º –∫–∞–∂–¥—ã–π —Å–ø–∞–Ω –Ω–∞ word-level
    word_level = []
    for s, e, tag in raw_entities:
        ent_type = tag.split("-", 1)[-1]
        word_level.extend(split_entity_into_bio_spans(text, s, e, ent_type))

    # 2) –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø—É—Å—Ç—ã—Ö –∏ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏
    filtered = []
    for s, e, tag in word_level:
        if s >= e:
            continue
        piece = text[s:e]
        if not piece or not piece.strip():
            continue
        if NON_WORD_RE.match(piece):
            continue
        filtered.append((s, e, tag))

    # 3) –°–ª–∏—è–Ω–∏–µ –ø–µ—Ä–µ—Å–µ–∫–∞—é—â–∏—Ö—Å—è/–≤–ª–æ–∂–µ–Ω–Ω—ã—Ö
    merged = merge_overlapping_spans(filtered)

    # 4) –°–æ—Ä—Ç–∏—Ä—É–µ–º –∏ –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ–º BIO –º–µ–∂–¥—É —Å–ª–æ–≤–∞–º–∏
    merged.sort(key=lambda x: (x[0], x[1]))
    final = []
    prev_type = None
    for s, e, tag in merged:
        curr_type = tag.split("-", 1)[-1]
        if prev_type == curr_type:
            final.append((s, e, f"I-{curr_type}"))
        else:
            final.append((s, e, f"B-{curr_type}"))
        prev_type = curr_type

    return final


# =====================
# üîπ –ú–æ–¥–µ–ª—å
# =====================
model = NERModel().to(device)
state = torch.load(CKPT_PATH, map_location=device)
model.load_state_dict(state)
model.eval()

# =====================
# üîπ –î–∞–Ω–Ω—ã–µ submission.csv
# =====================
df = pd.read_csv(INPUT_SUBMISSION, sep=";")

# –£–Ω–∏—Ñ–∏—Ü–∏—Ä—É–µ–º –∫–æ–ª–æ–Ω–∫–∏: sample ‚Üí search_query
if "search_query" not in df.columns and "sample" in df.columns:
    df = df.rename(columns={"sample": "search_query"})

# =====================
# üîπ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è + –ø–æ—Å—Ç–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥
# =====================
new_annotations = []

for _, row in df.iterrows():
    idx = row["id"] if "id" in df.columns else row.name
    original_query = row["search_query"]

    # –î–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫—É; –≤ —Ñ–∞–π–ª –ø–∏—à–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç
    text_for_model = preprocess_query(original_query)

    enc = tokenizer(text_for_model, return_offsets_mapping=True, add_special_tokens=False)
    input_ids = enc["input_ids"]
    offsets = enc["offset_mapping"]

    if not input_ids:
        new_annotations.append([])
        continue

    attn_mask = [1] * len(input_ids)
    input_ids_tensor = torch.tensor([input_ids], dtype=torch.long).to(device)
    mask_tensor = torch.tensor([attn_mask], dtype=torch.long).to(device)

    with torch.no_grad():
        pred_tag_idxs = model(input_ids_tensor, mask_tensor)[0]

    pred_tags = [id2label[tag_idx] for tag_idx in pred_tag_idxs]

    # --- —Å–æ–±–∏—Ä–∞–µ–º "—Å—ã—Ä–æ–π" —Å–ø–∏—Å–æ–∫ —Å—É—â–Ω–æ—Å—Ç–µ–π ---
    raw_entities = []
    current = None
    for tag, (start_char, end_char) in zip(pred_tags, offsets):
        if start_char == end_char:
            continue
        if tag.startswith("B-"):
            if current is not None:
                raw_entities.append((current[0], current[1], f"B-{current[2]}"))
            ent_type = tag.split("-", 1)[1]
            current = [start_char, end_char, ent_type]
        elif tag.startswith("I-"):
            ent_type = tag.split("-", 1)[1]
            if current is not None and current[2] == ent_type:
                current[1] = end_char
            else:
                current = [start_char, end_char, ent_type]
        else:  # O
            if current is not None:
                raw_entities.append((current[0], current[1], f"B-{current[2]}"))
                current = None
    if current is not None:
        raw_entities.append((current[0], current[1], f"B-{current[2]}"))

    # --- –ø–æ—Å—Ç–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥ ---
    final_entities = postprocess_to_word_level_bio(text_for_model, raw_entities)
    new_annotations.append(final_entities)

# =====================
# üîπ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
# =====================
df["annotation"] = new_annotations
cols = ["id", "search_query", "annotation"] if "id" in df.columns else ["search_query", "annotation"]
df.to_csv(OUTPUT_SUBMISSION, columns=cols, sep=";", index=False)

print(f"‚úÖ Saved submission with postprocessed predictions to {OUTPUT_SUBMISSION}")
