"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è submission_result.csv.
–í —Ä–µ–∑—É–ª—å—Ç–∏—Ä—É—é—â–µ–º —Ñ–∞–π–ª–µ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è —Ç–µ –∂–µ id –∏ search_query, —á—Ç–æ –∏ –≤ submission.csv,
–∞ annotation –∑–∞–º–µ–Ω—è–µ—Ç—Å—è –Ω–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–µ –º–æ–¥–µ–ª—å—é (word-level BIO).
"""

import os
import re
import torch
import pandas as pd
from data_processing import preprocess_query, tokenizer
from model import NERModel, id2label

# –ü—É—Ç–∏
DATA_DIR = "data"
INPUT_SUBMISSION = os.path.join(DATA_DIR, "submission.csv")   # –∏—Å—Ö–æ–¥–Ω—ã–π —Ñ–∞–π–ª
OUTPUT_SUBMISSION = os.path.join(DATA_DIR, "submission_result.csv")  # –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π —Ñ–∞–π–ª
CKPT_PATH = os.path.join("model_checkpoint", "ner_model.pth")

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# =====================
# üîπ –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ
# =====================
WORD_RE = re.compile(r"[0-9A-Za-z–ê-–Ø–∞-—è–Å—ë]+")


def split_entity_into_bio_spans(text: str, start: int, end: int, ent_type: str):
    """–†–∞–∑–±–∏–≤–∞–µ—Ç —Å—É—â–Ω–æ—Å—Ç—å –Ω–∞ BIO-—Å–ø–∞–Ω—ã –ø–æ —Å–ª–æ–≤–∞–º (–∫–∞–∫ –≤ train)."""
    chunk = text[start:end]
    spans = [(m.start() + start, m.end() + start) for m in WORD_RE.finditer(chunk)]
    if not spans:
        return [(start, end, f"B-{ent_type}")]
    bio = []
    for i, (s, e) in enumerate(spans):
        tag = "B" if i == 0 else "I"
        bio.append((s, e, f"{tag}-{ent_type}"))
    return bio


# =====================
# üîπ –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
# =====================
model = NERModel().to(device)
state = torch.load(CKPT_PATH, map_location=device)
model.load_state_dict(state)
model.eval()

# =====================
# üîπ –ó–∞–≥—Ä—É–∂–∞–µ–º submission.csv
# =====================
df = pd.read_csv(INPUT_SUBMISSION, sep=";")

# =====================
# üîπ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
# =====================
new_annotations = []

for _, row in df.iterrows():
    # —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ –ø–æ–ª—É—á–∞–µ–º id
    if "id" in df.columns:
        idx = row["id"]
    else:
        idx = row.name  # –∏–Ω–¥–µ–∫—Å —Å—Ç—Ä–æ–∫–∏, –µ—Å–ª–∏ –∫–æ–ª–æ–Ω–∫–∏ id –Ω–µ—Ç

    original_query = row["sample"]
    text = preprocess_query(original_query)

    enc = tokenizer(text, return_offsets_mapping=True, add_special_tokens=False)
    input_ids = enc["input_ids"]
    offsets = enc["offset_mapping"]

    if len(input_ids) == 0:
        new_annotations.append([])
        continue

    attn_mask = [1] * len(input_ids)
    input_ids_tensor = torch.tensor([input_ids], dtype=torch.long).to(device)
    mask_tensor = torch.tensor([attn_mask], dtype=torch.long).to(device)

    with torch.no_grad():
        pred_tag_idxs = model(input_ids_tensor, mask_tensor)[0]  # list[int]

    pred_tags = [id2label[tag_idx] for tag_idx in pred_tag_idxs]

    # === –í–æ—Å—Å—Ç–∞–Ω–æ–≤–∏–º word-level —Å—É—â–Ω–æ—Å—Ç–∏ ===
    entities_pred = []
    current = None

    for tag, (start_char, end_char) in zip(pred_tags, offsets):
        if start_char == end_char:
            continue
        if tag.startswith("B-"):
            if current is not None:
                entities_pred.extend(
                    split_entity_into_bio_spans(text, current[0], current[1], current[2])
                )
            ent_type = tag.split("-", 1)[1]
            current = [start_char, end_char, ent_type]
        elif tag.startswith("I-"):
            ent_type = tag.split("-", 1)[1]
            if current is not None and current[2] == ent_type:
                current[1] = end_char
            else:
                current = [start_char, end_char, ent_type]
        else:
            if current is not None:
                entities_pred.extend(
                    split_entity_into_bio_spans(text, current[0], current[1], current[2])
                )
                current = None

    if current is not None:
        entities_pred.extend(
            split_entity_into_bio_spans(text, current[0], current[1], current[2])
        )

    new_annotations.append(entities_pred)

# –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –æ–±—Ä–∞—Ç–Ω–æ
df["annotation"] = new_annotations
df.to_csv(OUTPUT_SUBMISSION, sep=";", index=False)

print(f"‚úÖ Saved submission with predictions to {OUTPUT_SUBMISSION}")
